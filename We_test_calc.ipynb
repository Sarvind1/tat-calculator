{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "13c11a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAT Calculator System - Enhanced with Excel Export\n",
      "Usage:\n",
      "1. calculator = TATCalculator()\n",
      "2. results = calculator.process_batch(df)\n",
      "3. calculator.export_to_excel(df, results, 'output.xlsx')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/q5x0b26j433crqf4l5qsxqc00000gn/T/ipykernel_18389/3236826720.py:40: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator('expression')\n",
      "/var/folders/cq/q5x0b26j433crqf4l5qsxqc00000gn/T/ipykernel_18389/3236826720.py:57: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator('preceding_stage')\n",
      "/var/folders/cq/q5x0b26j433crqf4l5qsxqc00000gn/T/ipykernel_18389/3236826720.py:69: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator('stages')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TAT (Turnaround Time) Calculator System\n",
    "=====================================\n",
    "\n",
    "A comprehensive system for calculating adjusted timestamps for Purchase Order (PO) \n",
    "processing workflow stages using a configurable, dependency-driven approach.\n",
    "\n",
    "This implementation replaces complex Excel formulas with maintainable Python code\n",
    "that supports dynamic expressions, stage dependencies, and transparent reasoning.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ProcessFlow(BaseModel):\n",
    "    \"\"\"Process flow metadata for a stage\"\"\"\n",
    "    critical_path: bool\n",
    "    parallel_processes: List[str] = Field(default_factory=list)\n",
    "    handoff_points: List[str] = Field(default_factory=list)\n",
    "    process_type: str\n",
    "    team_owner: str\n",
    "\n",
    "\n",
    "class FallbackCalculation(BaseModel):\n",
    "    \"\"\"Fallback calculation configuration\"\"\"\n",
    "    expression: str\n",
    "    \n",
    "    @validator('expression')\n",
    "    def validate_expression(cls, v):\n",
    "        \"\"\"Validate that expression is not empty\"\"\"\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"Expression cannot be empty\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class StageConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single stage\"\"\"\n",
    "    name: str\n",
    "    actual_timestamp: Optional[str] = None\n",
    "    preceding_stage: Optional[Union[str, List[str]]] = None\n",
    "    process_flow: ProcessFlow\n",
    "    fallback_calculation: FallbackCalculation\n",
    "    lead_time: int = Field(ge=0, description=\"Lead time in days\")\n",
    "    \n",
    "    @validator('preceding_stage')\n",
    "    def validate_preceding_stage(cls, v):\n",
    "        \"\"\"Convert single string to list for consistency\"\"\"\n",
    "        if isinstance(v, str):\n",
    "            return [v]\n",
    "        return v\n",
    "\n",
    "\n",
    "class StagesConfig(BaseModel):\n",
    "    \"\"\"Complete stages configuration\"\"\"\n",
    "    stages: Dict[str, StageConfig]\n",
    "    \n",
    "    @validator('stages')\n",
    "    def validate_stage_ids(cls, v):\n",
    "        \"\"\"Ensure all stage IDs are valid\"\"\"\n",
    "        for stage_id in v.keys():\n",
    "            if not stage_id.strip():\n",
    "                raise ValueError(\"Stage ID cannot be empty\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class TATCalculator:\n",
    "    \"\"\"\n",
    "    Core TAT calculation engine that processes PO data through configurable stages.\n",
    "    \n",
    "    Features:\n",
    "    - Configuration-driven stage definitions\n",
    "    - Dynamic expression evaluation with custom functions\n",
    "    - Dependency resolution with cycle detection\n",
    "    - Memoization for performance optimization\n",
    "    - Comprehensive audit trails and reasoning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = \"stages_config.json\"):\n",
    "        \"\"\"\n",
    "        Initialize the TAT Calculator\n",
    "        \n",
    "        Args:\n",
    "            config_path: Path to the stages configuration JSON file\n",
    "        \"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self._validate_config()\n",
    "        self.calculated_adjustments: Dict[str, Tuple[Optional[datetime], Dict[str, Any]]] = {}\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> StagesConfig:\n",
    "        \"\"\"Load and validate configuration from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "            return StagesConfig(**config_data)\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Configuration file not found: {config_path}\")\n",
    "            raise\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Invalid JSON in configuration file: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading configuration: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate configuration for circular dependencies\"\"\"\n",
    "        # Build dependency graph\n",
    "        graph = {}\n",
    "        for stage_id, stage in self.config.stages.items():\n",
    "            graph[stage_id] = stage.preceding_stage or []\n",
    "        \n",
    "        # Check for cycles using DFS\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        \n",
    "        def has_cycle(node):\n",
    "            if node in rec_stack:\n",
    "                return True\n",
    "            if node in visited:\n",
    "                return False\n",
    "                \n",
    "            visited.add(node)\n",
    "            rec_stack.add(node)\n",
    "            \n",
    "            for neighbor in graph.get(node, []):\n",
    "                if neighbor in graph and has_cycle(neighbor):\n",
    "                    return True\n",
    "            \n",
    "            rec_stack.remove(node)\n",
    "            return False\n",
    "        \n",
    "        for stage_id in graph:\n",
    "            if stage_id not in visited:\n",
    "                if has_cycle(stage_id):\n",
    "                    raise ValueError(f\"Circular dependency detected involving stage {stage_id}\")\n",
    "    \n",
    "    def _get_date_value(self, field_name: str, po_row: pd.Series) -> Optional[datetime]:\n",
    "        \"\"\"\n",
    "        Extract datetime value from PO data with robust handling\n",
    "        \n",
    "        Args:\n",
    "            field_name: Name of the field to extract\n",
    "            po_row: Pandas Series containing PO data\n",
    "            \n",
    "        Returns:\n",
    "            datetime object or None if not available/valid\n",
    "        \"\"\"\n",
    "        if field_name not in po_row.index:\n",
    "            logger.warning(f\"Field '{field_name}' not found in PO data\")\n",
    "            return None\n",
    "            \n",
    "        value = po_row[field_name]\n",
    "        \n",
    "        # Handle various data types\n",
    "        if pd.isna(value) or value == \"\" or value == \"NA\":\n",
    "            return None\n",
    "            \n",
    "        if isinstance(value, datetime):\n",
    "            return value\n",
    "            \n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                # Try common datetime formats\n",
    "                for fmt in [\"%Y-%m-%d\", \"%Y-%m-%d %H:%M:%S\", \"%m/%d/%Y\", \"%d/%m/%Y\"]:\n",
    "                    try:\n",
    "                        return datetime.strptime(value, fmt)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                        \n",
    "                # Try pandas datetime parsing as fallback\n",
    "                return pd.to_datetime(value)\n",
    "            except:\n",
    "                logger.warning(f\"Could not parse date from field '{field_name}': {value}\")\n",
    "                return None\n",
    "                \n",
    "        return None\n",
    "    \n",
    "    def _evaluate_expression(self, expression: str, po_row: pd.Series) -> Tuple[Optional[datetime], str]:\n",
    "        try:\n",
    "            tree = ast.parse(expression, mode='eval')\n",
    "            result = self._eval_node(tree.body, po_row)\n",
    "            \n",
    "            if isinstance(result, datetime):\n",
    "                return result, f\"Calculation: {expression} = {result.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "            elif result is not None:\n",
    "                # Handle non-datetime results (for testing)\n",
    "                return None, f\"Calculation returned non-datetime: {expression} = {result}\"\n",
    "            else:\n",
    "                return None, f\"Calculation failed: {expression}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating expression '{expression}': {e}\")\n",
    "            return None, f\"Calculation error: {expression} ({str(e)})\"\n",
    "    \n",
    "\n",
    "    def _eval_node(self, node: ast.AST, po_row: pd.Series) -> Any:\n",
    "        \"\"\"Recursively evaluate AST nodes\"\"\"\n",
    "        if isinstance(node, ast.Name):\n",
    "            return po_row.get(node.id)\n",
    "        \n",
    "        elif isinstance(node, ast.Constant):\n",
    "            return node.value\n",
    "        \n",
    "        elif isinstance(node, ast.List):\n",
    "            # Handle list literals like ['5'], ['2']\n",
    "            return [self._eval_node(elt, po_row) for elt in node.elts]\n",
    "        \n",
    "        elif isinstance(node, ast.Compare):\n",
    "            # Handle comparisons like pi_applicable==1\n",
    "            left = self._eval_node(node.left, po_row)\n",
    "            op = node.ops[0]\n",
    "            right = self._eval_node(node.comparators[0], po_row)\n",
    "            \n",
    "            if isinstance(op, ast.Eq):\n",
    "                return left == right\n",
    "            elif isinstance(op, ast.NotEq):\n",
    "                return left != right\n",
    "            elif isinstance(op, ast.Lt):\n",
    "                return left < right\n",
    "            elif isinstance(op, ast.LtE):\n",
    "                return left <= right\n",
    "            elif isinstance(op, ast.Gt):\n",
    "                return left > right\n",
    "            elif isinstance(op, ast.GtE):\n",
    "                return left >= right\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported comparison operator: {type(op).__name__}\")\n",
    "        \n",
    "        elif isinstance(node, ast.Call):\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                func_name = node.func.id\n",
    "                \n",
    "                # Handle iff with lazy evaluation\n",
    "                if func_name == 'iff':\n",
    "                    if len(node.args) == 3:\n",
    "                        condition = self._eval_node(node.args[0], po_row)\n",
    "                        if condition:\n",
    "                            return self._eval_node(node.args[1], po_row)\n",
    "                        else:\n",
    "                            return self._eval_node(node.args[2], po_row)\n",
    "                    raise ValueError(\"iff requires exactly 3 arguments\")\n",
    "                \n",
    "                # For other functions, evaluate arguments first\n",
    "                args = [self._eval_node(arg, po_row) for arg in node.args]\n",
    "                \n",
    "                if func_name == 'max':\n",
    "                    valid_dates = [arg for arg in args if isinstance(arg, datetime)]\n",
    "                    return max(valid_dates) if valid_dates else None\n",
    "                \n",
    "                elif func_name == 'add_days':\n",
    "                    if len(args) >= 2 and isinstance(args[0], datetime) and isinstance(args[1], (int, float)):\n",
    "                        return args[0] + timedelta(days=int(args[1]))\n",
    "                    return None\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown function: {func_name}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported AST node type: {type(node).__name__}\")\n",
    "\n",
    "    def calculate_adjusted_timestamp(self, stage_id: str, po_row: pd.Series) -> Tuple[Optional[datetime], Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Calculate adjusted timestamp for a specific stage using priority logic:\n",
    "        \n",
    "        1. Precedence-based calculation (from dependencies + lead time)\n",
    "        2. Actual timestamp comparison (if available)\n",
    "        3. Fallback calculation (last resort)\n",
    "        \n",
    "        Args:\n",
    "            stage_id: ID of the stage to calculate\n",
    "            po_row: PO data row\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (calculated_timestamp, calculation_details)\n",
    "        \"\"\"\n",
    "        # Check if already calculated (memoization)\n",
    "        if stage_id in self.calculated_adjustments:\n",
    "            return self.calculated_adjustments[stage_id]\n",
    "        \n",
    "        if stage_id not in self.config.stages:\n",
    "            logger.error(f\"Stage {stage_id} not found in configuration\")\n",
    "            return None, {\"method\": \"error\", \"reason\": f\"Stage {stage_id} not found\"}\n",
    "        \n",
    "        stage = self.config.stages[stage_id]\n",
    "        \n",
    "        # Initialize calculation details\n",
    "        calc_details = {\n",
    "            \"method\": None,\n",
    "            \"source\": None,\n",
    "            \"base_date\": None,\n",
    "            \"lead_time_applied\": stage.lead_time,\n",
    "            \"decision_reason\": None,\n",
    "            \"dependencies\": [],\n",
    "            \"actual_field\": None,\n",
    "            \"actual_value\": None,\n",
    "            \"precedence_value\": None,\n",
    "            \"final_choice\": None\n",
    "        }\n",
    "        \n",
    "        # 1. Calculate precedence-based timestamp\n",
    "        precedence_timestamp = None\n",
    "        if stage.preceding_stage:\n",
    "            dependencies = []\n",
    "            preceding_timestamps = []\n",
    "            \n",
    "            # Check if it's a single conditional expression\n",
    "            if len(stage.preceding_stage) == 1 and isinstance(stage.preceding_stage[0], str) and 'iff(' in stage.preceding_stage[0]:\n",
    "                # Evaluate the conditional to get the list of stage IDs\n",
    "                try:\n",
    "                    result = self._eval_node(ast.parse(stage.preceding_stage[0], mode='eval').body, po_row)\n",
    "                    preceding_stage_ids = result if isinstance(result, list) else [result] if result else []\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error evaluating conditional preceding stage: {e}\")\n",
    "                    preceding_stage_ids = []\n",
    "            else:\n",
    "                preceding_stage_ids = stage.preceding_stage\n",
    "            \n",
    "            # Process the stage IDs\n",
    "            for prec_stage_id in preceding_stage_ids:\n",
    "                prec_stage_id = str(prec_stage_id)  # Ensure it's a string\n",
    "                if prec_stage_id in self.config.stages:\n",
    "                    prec_timestamp, prec_details = self.calculate_adjusted_timestamp(prec_stage_id, po_row)\n",
    "                    if prec_timestamp:\n",
    "                        preceding_timestamps.append(prec_timestamp)\n",
    "                        dependencies.append({\n",
    "                            \"stage_id\": prec_stage_id,\n",
    "                            \"stage_name\": self.config.stages[prec_stage_id].name,\n",
    "                            \"timestamp\": prec_timestamp.isoformat(),\n",
    "                            \"method\": prec_details[\"method\"] if isinstance(prec_details, dict) else \"legacy\"\n",
    "                        })\n",
    "            \n",
    "            calc_details[\"dependencies\"] = dependencies\n",
    "            if preceding_timestamps:\n",
    "                base_timestamp = max(preceding_timestamps)\n",
    "                precedence_timestamp = base_timestamp + timedelta(days=stage.lead_time)\n",
    "                calc_details[\"precedence_value\"] = precedence_timestamp.isoformat()\n",
    "                calc_details[\"base_date\"] = base_timestamp.isoformat()\n",
    "        \n",
    "        # 2. Extract and get actual timestamp (evaluate as expression or field)\n",
    "        actual_timestamp = None\n",
    "        actual_formula = None\n",
    "        if stage.actual_timestamp:\n",
    "            actual_timestamp, actual_formula = self._evaluate_expression(stage.actual_timestamp, po_row)\n",
    "            calc_details[\"actual_field\"] = stage.actual_timestamp\n",
    "            if actual_timestamp:\n",
    "                calc_details[\"actual_value\"] = actual_timestamp.isoformat()\n",
    "        \n",
    "        # 3. Determine final timestamp and method\n",
    "        final_timestamp = None\n",
    "        if precedence_timestamp and actual_timestamp:\n",
    "            if actual_timestamp >= precedence_timestamp:\n",
    "                final_timestamp = actual_timestamp\n",
    "                calc_details[\"method\"] = \"actual_over_precedence\"\n",
    "                calc_details[\"source\"] = actual_formula\n",
    "                calc_details[\"decision_reason\"] = f\"Actual date ({actual_timestamp.strftime('%Y-%m-%d')}) is later than precedence date ({precedence_timestamp.strftime('%Y-%m-%d')})\"\n",
    "                calc_details[\"final_choice\"] = \"actual\"\n",
    "            else:\n",
    "                final_timestamp = precedence_timestamp\n",
    "                calc_details[\"method\"] = \"precedence_over_actual\"\n",
    "                calc_details[\"source\"] = f\"Calculated from dependencies\"\n",
    "                calc_details[\"decision_reason\"] = f\"Precedence stage's timestamp ({precedence_timestamp.strftime('%Y-%m-%d')}) is later than actual ({actual_timestamp.strftime('%Y-%m-%d')})\"\n",
    "                calc_details[\"final_choice\"] = \"precedence\"\n",
    "        \n",
    "        elif actual_timestamp:\n",
    "            final_timestamp = actual_timestamp\n",
    "            calc_details[\"method\"] = \"actual_only\"\n",
    "            calc_details[\"source\"] = actual_formula\n",
    "            calc_details[\"decision_reason\"] = \"Using actual timestamp\"\n",
    "            calc_details[\"final_choice\"] = \"actual\"\n",
    "        \n",
    "        elif precedence_timestamp:\n",
    "            final_timestamp = precedence_timestamp\n",
    "            calc_details[\"method\"] = \"precedence_only\"\n",
    "            calc_details[\"source\"] = f\"Calculated from dependencies + {stage.lead_time} days\"\n",
    "            calc_details[\"decision_reason\"] = \"No actual timestamp available, using precedence calculation\"\n",
    "            calc_details[\"final_choice\"] = \"precedence\"\n",
    "        \n",
    "        # 4. Fallback calculation if no valid timestamp\n",
    "        if not final_timestamp:\n",
    "            fallback_result, fallback_formula = self._evaluate_expression(\n",
    "                stage.fallback_calculation.expression, po_row\n",
    "            )\n",
    "            if fallback_result:\n",
    "                final_timestamp = fallback_result + timedelta(days=stage.lead_time)\n",
    "                calc_details[\"method\"] = \"fallback\"\n",
    "                calc_details[\"source\"] = stage.fallback_calculation.expression\n",
    "                calc_details[\"base_date\"] = fallback_result.isoformat()\n",
    "                calc_details[\"decision_reason\"] = \"No precedence or actual data available, using fallback expression\"\n",
    "                calc_details[\"final_choice\"] = \"fallback\"\n",
    "            else:\n",
    "                calc_details[\"method\"] = \"failed\"\n",
    "                calc_details[\"decision_reason\"] = \"No valid calculation method available\"\n",
    "        \n",
    "        # Cache result\n",
    "        result = (final_timestamp, calc_details)\n",
    "        self.calculated_adjustments[stage_id] = result\n",
    "        \n",
    "        return result\n",
    "    def _extract_actual_field(self, expression: str) -> Optional[str]:\n",
    "        \"\"\"Extract the 'actual' field from a max() expression\"\"\"\n",
    "        # This method should be at the same level as _eval_node, not inside it\n",
    "        try:\n",
    "            tree = ast.parse(expression, mode='eval')\n",
    "            if isinstance(tree.body, ast.Call) and isinstance(tree.body.func, ast.Name):\n",
    "                if tree.body.func.id == 'max' and tree.body.args:\n",
    "                    first_arg = tree.body.args[0]\n",
    "                    if isinstance(first_arg, ast.Name):\n",
    "                        return first_arg.id\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "        \n",
    "\n",
    "    \n",
    "    def calculate_tat(self, po_row: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calculate TAT for all stages of a PO\n",
    "        \n",
    "        Args:\n",
    "            po_row: Pandas Series containing PO data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with complete TAT calculation results\n",
    "        \"\"\"\n",
    "        # Clear cache for new calculation\n",
    "        self.calculated_adjustments = {}\n",
    "        \n",
    "        result = {\n",
    "            \"po_id\": po_row.get('po_razin_id', 'Unknown'),\n",
    "            \"calculation_date\": datetime.now().isoformat(),\n",
    "            \"summary\": {\n",
    "                \"total_stages\": len(self.config.stages),\n",
    "                \"calculated_stages\": 0,\n",
    "                \"methods_used\": {\n",
    "                    \"actual_only\": 0,\n",
    "                    \"precedence_only\": 0,\n",
    "                    \"actual_over_precedence\": 0,\n",
    "                    \"precedence_over_actual\": 0,\n",
    "                    \"fallback\": 0,\n",
    "                    \"failed\": 0\n",
    "                }\n",
    "            },\n",
    "            \"stages\": {}\n",
    "        }\n",
    "        \n",
    "        # Calculate each stage\n",
    "        for stage_id, stage_config in self.config.stages.items():\n",
    "            timestamp, calc_details = self.calculate_adjusted_timestamp(stage_id, po_row)\n",
    "            \n",
    "            # Update summary statistics\n",
    "            if timestamp:\n",
    "                result[\"summary\"][\"calculated_stages\"] += 1\n",
    "            \n",
    "            method = calc_details.get(\"method\", \"unknown\") if isinstance(calc_details, dict) else \"legacy\"\n",
    "            if method in result[\"summary\"][\"methods_used\"]:\n",
    "                result[\"summary\"][\"methods_used\"][method] += 1\n",
    "            \n",
    "            # Create clean stage result\n",
    "            stage_result = {\n",
    "                \"name\": stage_config.name,\n",
    "                \"timestamp\": timestamp.isoformat() if timestamp else None,\n",
    "                \"calculation\": self._format_calculation_summary(calc_details, stage_config),\n",
    "                \"process_flow\": {\n",
    "                    \"team_owner\": stage_config.process_flow.team_owner,\n",
    "                    \"process_type\": stage_config.process_flow.process_type,\n",
    "                    \"critical_path\": stage_config.process_flow.critical_path,\n",
    "                    \"handoff_points\": stage_config.process_flow.handoff_points\n",
    "                },\n",
    "                \"dependencies\": calc_details.get(\"dependencies\", []) if isinstance(calc_details, dict) else []\n",
    "            }\n",
    "            \n",
    "            result[\"stages\"][stage_id] = stage_result\n",
    "        \n",
    "        # Calculate completion rate\n",
    "        result[\"summary\"][\"completion_rate\"] = round(\n",
    "            result[\"summary\"][\"calculated_stages\"] / result[\"summary\"][\"total_stages\"] * 100, 2\n",
    "        ) if result[\"summary\"][\"total_stages\"] > 0 else 0\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _format_calculation_summary(self, calc_details: Dict[str, Any], stage_config: StageConfig) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Format calculation details into a clean, readable summary\n",
    "        \n",
    "        Args:\n",
    "            calc_details: Raw calculation details\n",
    "            stage_config: Stage configuration\n",
    "            \n",
    "        Returns:\n",
    "            Clean calculation summary\n",
    "        \"\"\"\n",
    "        if not isinstance(calc_details, dict):\n",
    "            return {\"method\": \"legacy\", \"summary\": str(calc_details)}\n",
    "        \n",
    "        method = calc_details.get(\"method\", \"unknown\")\n",
    "        \n",
    "        summary = {\n",
    "            \"method\": method,\n",
    "            \"source\": calc_details.get(\"source\"),\n",
    "            \"decision\": calc_details.get(\"decision_reason\"),\n",
    "            \"lead_time_days\": calc_details.get(\"lead_time_applied\", 0)\n",
    "        }\n",
    "        \n",
    "        # Add method-specific details\n",
    "        if method == \"actual_over_precedence\":\n",
    "            summary.update({\n",
    "                \"actual_date\": calc_details.get(\"actual_value\"),\n",
    "                \"precedence_date\": calc_details.get(\"precedence_value\"),\n",
    "                \"reason\": \"Actual timestamp is later than calculated precedence\"\n",
    "            })\n",
    "        elif method == \"precedence_over_actual\":\n",
    "            summary.update({\n",
    "                \"actual_date\": calc_details.get(\"actual_value\"),\n",
    "                \"precedence_date\": calc_details.get(\"precedence_value\"), \n",
    "                \"reason\": \"Calculated precedence is later than actual timestamp\"\n",
    "            })\n",
    "        elif method == \"precedence_only\":\n",
    "            summary.update({\n",
    "                \"base_date\": calc_details.get(\"base_date\"),\n",
    "                \"reason\": \"No actual timestamp available\"\n",
    "            })\n",
    "        elif method == \"actual_only\":\n",
    "            summary.update({\n",
    "                \"actual_field\": calc_details.get(\"actual_field\"),\n",
    "                \"reason\": \"No dependency chain available\"\n",
    "            })\n",
    "        elif method == \"fallback\":\n",
    "            summary.update({\n",
    "                \"base_date\": calc_details.get(\"base_date\"),\n",
    "                \"expression\": calc_details.get(\"source\"),\n",
    "                \"reason\": \"Using fallback calculation\"\n",
    "            })\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def process_batch(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process multiple POs in batch\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame containing multiple PO rows\n",
    "            \n",
    "        Returns:\n",
    "            List of TAT calculation results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                result = self.calculate_tat(row)\n",
    "                results.append(result)\n",
    "                logger.info(f\"Processed PO: {result['po_id']}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {index}: {e}\")\n",
    "                results.append({\n",
    "                    \"po_id\": row.get('po_razin_id', f'Row_{index}'),\n",
    "                    \"error\": str(e),\n",
    "                    \"calculation_date\": datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def export_to_excel(self, df: pd.DataFrame, results: List[Dict[str, Any]], output_file: str):\n",
    "        \"\"\"\n",
    "        Export original data + calculated timestamps to Excel\n",
    "        \n",
    "        Args:\n",
    "            df: Original DataFrame\n",
    "            results: TAT calculation results\n",
    "            output_file: Output Excel file path\n",
    "        \"\"\"\n",
    "        # Create a copy of the original dataframe\n",
    "        export_df = df.copy()\n",
    "        \n",
    "        # Add calculated timestamps for each stage\n",
    "        for result in results:\n",
    "            if 'stages' not in result:\n",
    "                continue\n",
    "                \n",
    "            po_id = result['po_id']\n",
    "            po_index = export_df[export_df['po_razin_id'] == po_id].index\n",
    "            \n",
    "            if len(po_index) > 0:\n",
    "                idx = po_index[0]\n",
    "                \n",
    "                # Add calculated timestamps\n",
    "                for stage_id, stage_data in result['stages'].items():\n",
    "                    # Use stage name instead of ID for column name\n",
    "                    stage_name = stage_data['name']\n",
    "                    col_name = f\"{stage_name}_Date\"\n",
    "                    timestamp = stage_data['timestamp']\n",
    "                    # Convert timestamp to date only\n",
    "                    if timestamp:\n",
    "                        date = pd.to_datetime(timestamp).date()\n",
    "                        export_df.loc[idx, col_name] = date\n",
    "                    else:\n",
    "                        export_df.loc[idx, col_name] = None\n",
    "        \n",
    "        # Save to Excel\n",
    "        export_df.to_excel(output_file, index=False)\n",
    "        logger.info(f\"Results exported to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"TAT Calculator System - Enhanced with Excel Export\")\n",
    "    print(\"Usage:\")\n",
    "    print(\"1. calculator = TATCalculator()\")\n",
    "    print(\"2. results = calculator.process_batch(df)\")\n",
    "    print(\"3. calculator.export_to_excel(df, results, 'output.xlsx')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f5495",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (837283707.py, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[219], line 90\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.df[col] = self.df[col].mask(pd.isna(self.df[col]), None)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete TAT Calculation Runner\n",
    "==============================\n",
    "\n",
    "This script runs the complete TAT calculation system on your Excel data\n",
    "and generates comprehensive reports and analytics.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# Set up comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('tat_calculation.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TATRunner:\n",
    "    \"\"\"Complete TAT calculation runner with enhanced reporting\"\"\"\n",
    "    \n",
    "    def __init__(self, excel_file: str = \"ts_big.xlsx\", config_file: str = \"stages_config.json\"):\n",
    "        self.excel_file = excel_file\n",
    "        self.config_file = config_file\n",
    "        self.df = None\n",
    "        self.calculator = None\n",
    "        self.results = []\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Set up the calculator and load data\"\"\"\n",
    "        logger.info(\"Setting up TAT calculation environment...\")\n",
    "        \n",
    "        # Load Excel data\n",
    "        self.load_excel_data()\n",
    "        \n",
    "        # Initialize calculator\n",
    "        self.initialize_calculator()\n",
    "        \n",
    "        logger.info(\"Setup completed successfully!\")\n",
    "        \n",
    "    def load_excel_data(self):\n",
    "        \"\"\"Load and prepare Excel data\"\"\"\n",
    "        logger.info(f\"Loading Excel file: {self.excel_file}\")\n",
    "        \n",
    "        try:\n",
    "            self.df = pd.read_excel(self.excel_file)\n",
    "            logger.info(f\"Loaded {len(self.df)} rows and {len(self.df.columns)} columns\")\n",
    "            \n",
    "            # Clean column names\n",
    "            self.df.columns = self.df.columns.str.strip()\n",
    "            \n",
    "            # Convert date columns\n",
    "            self.convert_date_columns()\n",
    "            \n",
    "            # Validate required columns\n",
    "            self.validate_required_columns()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Excel file: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def convert_date_columns(self):\n",
    "        \"\"\"Convert relevant columns to datetime\"\"\"\n",
    "        date_columns = [\n",
    "            'po_created_date', 'po_approval_date', 'supplier_confirmation_date',\n",
    "            'pi_invoice_approval_date', 'pi_payment_date', 'receive_first_prd_date',\n",
    "            'prd_reconfirmed_date', 'po_im_date_value', 'po_sm_date_value',\n",
    "            'batch_created_ts', 'sm_signoff_ts', 'ci_invoice_approval_date',\n",
    "            'ci_payment_date', 'qc_schedule_date', 'ffw_booking_ts', 'spd_ts',\n",
    "            'stock_pickup_date', 'shipment_creation_date', 'shipment_in_transit_date',\n",
    "            'bi_invoice_approval_date', 'bi_payment_date', 'ffwp_telex_release_date',\n",
    "            'shipment_stock_delivery_date', 'item_receipt_date', 'actual_cargo_pick_up_date',\n",
    "            'actual_shipping_date', 'actual_arrival_date', 'actual_delivery_date'\n",
    "        ]\n",
    "        \n",
    "        for col in date_columns:\n",
    "            if col in self.df.columns:\n",
    "                original_type = self.df[col].dtype\n",
    "                self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "                self.df[col] = self.df[col].mask(pd.isna(self.df[col]), None)\n",
    "\n",
    "                \n",
    "                missing_count = self.df[col].isna().sum()\n",
    "                total_count = len(self.df)\n",
    "                \n",
    "                logger.info(f\"Converted {col}: {original_type} -> datetime, \"\n",
    "                          f\"{missing_count}/{total_count} missing ({missing_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    def validate_required_columns(self):\n",
    "        \"\"\"Validate that required columns exist\"\"\"\n",
    "        required_columns = ['po_razin_id', 'po_created_date']\n",
    "        missing_columns = [col for col in required_columns if col not in self.df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        logger.info(\"All required columns present\")\n",
    "    \n",
    "    def initialize_calculator(self):\n",
    "        \"\"\"Initialize the TAT calculator\"\"\"\n",
    "        logger.info(f\"Initializing TAT Calculator with config: {self.config_file}\")\n",
    "        \n",
    "        try:\n",
    "            self.calculator = TATCalculator(self.config_file)\n",
    "            logger.info(\"TAT Calculator initialized successfully\")\n",
    "            \n",
    "        except ImportError:\n",
    "            logger.error(\"TATCalculator not available. Please ensure tat_calculator.py is in the same directory.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing calculator: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_calculations(self, sample_size: int = None):\n",
    "        \"\"\"Run TAT calculations on all or sample of POs\"\"\"\n",
    "        if sample_size:\n",
    "            df_to_process = self.df.head(sample_size)\n",
    "            logger.info(f\"Processing sample of {sample_size} POs\")\n",
    "        else:\n",
    "            df_to_process = self.df\n",
    "            logger.info(f\"Processing all {len(df_to_process)} POs\")\n",
    "        \n",
    "        self.results = []\n",
    "        errors = []\n",
    "        \n",
    "        for index, row in df_to_process.iterrows():\n",
    "            try:\n",
    "                po_id = row.get('po_razin_id', f'Row_{index}')\n",
    "                logger.info(f\"Processing PO {index + 1}/{len(df_to_process)}: {po_id}\")\n",
    "                \n",
    "                result = self.calculator.calculate_tat(row)\n",
    "                self.results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_info = {\n",
    "                    'index': index,\n",
    "                    'po_id': row.get('po_razin_id', f'Row_{index}'),\n",
    "                    'error': str(e),\n",
    "                    'traceback': traceback.format_exc()\n",
    "                }\n",
    "                errors.append(error_info)\n",
    "                logger.error(f\"Error processing row {index}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Completed calculations: {len(self.results)} successful, {len(errors)} errors\")\n",
    "        \n",
    "        if errors:\n",
    "            self.save_errors(errors)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_errors(self, errors):\n",
    "        \"\"\"Save error details to file\"\"\"\n",
    "        error_file = f\"tat_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(error_file, 'w') as f:\n",
    "            json.dump(errors, f, indent=2)\n",
    "        logger.info(f\"Error details saved to: {error_file}\")\n",
    "    \n",
    "    def save_results(self, filename_prefix: str = \"tat_results\"):\n",
    "        \"\"\"Save calculation results to JSON file\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to save\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{filename_prefix}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Results saved to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_to_excel(self, filename_prefix: str = \"tat_export\"):\n",
    "        \"\"\"Export original data + calculated timestamps to Excel\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to export\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{filename_prefix}_{timestamp}.xlsx\"\n",
    "        \n",
    "        self.calculator.export_to_excel(self.df, self.results, filename)\n",
    "        return filename\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"TAT Calculation System - Starting...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize runner\n",
    "        runner = TATRunner()\n",
    "        \n",
    "        # Setup\n",
    "        runner.setup()\n",
    "        \n",
    "        # Run calculations (start with sample for testing)\n",
    "        sample_size = 5  # Process first 5 POs for testing\n",
    "        print(f\"\\nRunning calculations on sample of {sample_size} POs...\")\n",
    "        results = runner.run_calculations(sample_size=sample_size)\n",
    "        \n",
    "        if results:\n",
    "            # Save results\n",
    "            results_file = runner.save_results()\n",
    "            \n",
    "            # Export to Excel\n",
    "            excel_file = runner.export_to_excel()\n",
    "            \n",
    "            print(f\"\\nFiles Generated:\")\n",
    "            print(f\"- Results: {results_file}\")\n",
    "            print(f\"- Excel Export: {excel_file}\")\n",
    "            print(f\"- Logs: tat_calculation.log\")\n",
    "            \n",
    "        print(f\"\\nTo process all POs, change sample_size=None in the run_calculations() call\")\n",
    "        print(\"TAT Calculation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"See tat_calculation.log for detailed error information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea313501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Setting up TAT calculation environment...\n",
      "INFO:__main__:Loading Excel file: ts_big.xlsx\n",
      "INFO:__main__:Loaded 5 rows and 38 columns\n",
      "INFO:__main__:Converted po_created_date: datetime64[ns] -> datetime, 0/5 missing (0.0%)\n",
      "INFO:__main__:Converted po_approval_date: datetime64[ns] -> datetime, 0/5 missing (0.0%)\n",
      "INFO:__main__:Converted supplier_confirmation_date: datetime64[ns] -> datetime, 0/5 missing (0.0%)\n",
      "INFO:__main__:Converted pi_invoice_approval_date: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted pi_payment_date: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted receive_first_prd_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted prd_reconfirmed_date: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted po_im_date_value: datetime64[ns] -> datetime, 1/5 missing (20.0%)\n",
      "INFO:__main__:Converted po_sm_date_value: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted batch_created_ts: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted sm_signoff_ts: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted ci_invoice_approval_date: float64 -> datetime, 5/5 missing (100.0%)\n",
      "INFO:__main__:Converted ci_payment_date: float64 -> datetime, 5/5 missing (100.0%)\n",
      "INFO:__main__:Converted qc_schedule_date: float64 -> datetime, 5/5 missing (100.0%)\n",
      "INFO:__main__:Converted ffw_booking_ts: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted spd_ts: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted stock_pickup_date: float64 -> datetime, 5/5 missing (100.0%)\n",
      "INFO:__main__:Converted shipment_creation_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted shipment_in_transit_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted bi_invoice_approval_date: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted bi_payment_date: datetime64[ns] -> datetime, 3/5 missing (60.0%)\n",
      "INFO:__main__:Converted ffwp_telex_release_date: float64 -> datetime, 5/5 missing (100.0%)\n",
      "INFO:__main__:Converted shipment_stock_delivery_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted item_receipt_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted actual_cargo_pick_up_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted actual_shipping_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted actual_arrival_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:Converted actual_delivery_date: datetime64[ns] -> datetime, 2/5 missing (40.0%)\n",
      "INFO:__main__:All required columns present\n",
      "INFO:__main__:Initializing TAT Calculator with config: stages_config.json\n",
      "INFO:__main__:TAT Calculator initialized successfully\n",
      "INFO:__main__:Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "runner = TATRunner()\n",
    "runner.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = next(runner.df.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74ea9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "4d387c3a-566e-4aea-ab3f-cacf75f8ec7c",
       "rows": [
        [
         "po_razin_id",
         "PO356103HKGA-0000015"
        ],
        [
         "po_created_date",
         "2023-06-12 00:00:00"
        ],
        [
         "po_approval_date",
         "2023-06-19 00:00:00"
        ],
        [
         "supplier_confirmation_date",
         "2023-06-21 00:00:00"
        ],
        [
         "pi_invoice_approval_date",
         "2023-06-25 00:00:00"
        ],
        [
         "pi_payment_date",
         "2023-08-23 00:00:00"
        ],
        [
         "receive_first_prd_date",
         "2024-06-27 00:00:00"
        ],
        [
         "prd_reconfirmed_date",
         null
        ],
        [
         "po_im_date_value",
         "2024-02-22 00:00:00"
        ],
        [
         "po_sm_date_value",
         "2024-02-22 00:00:00"
        ],
        [
         "batch_created_ts",
         null
        ],
        [
         "sm_signoff_ts",
         null
        ],
        [
         "ci_invoice_approval_date",
         null
        ],
        [
         "ci_payment_date",
         null
        ],
        [
         "qc_schedule_date",
         null
        ],
        [
         "ffw_booking_ts",
         null
        ],
        [
         "spd_ts",
         null
        ],
        [
         "stock_pickup_date",
         null
        ],
        [
         "shipment_creation_date",
         null
        ],
        [
         "shipment_in_transit_date",
         null
        ],
        [
         "bi_invoice_approval_date",
         null
        ],
        [
         "bi_payment_date",
         null
        ],
        [
         "ffwp_telex_release_date",
         null
        ],
        [
         "shipment_stock_delivery_date",
         null
        ],
        [
         "item_receipt_date",
         null
        ],
        [
         "actual_cargo_pick_up_date",
         null
        ],
        [
         "actual_shipping_date",
         null
        ],
        [
         "actual_arrival_date",
         null
        ],
        [
         "actual_delivery_date",
         null
        ],
        [
         "first_prd_date",
         "2025-01-01 00:00:00"
        ],
        [
         "final_prd_date",
         "2025-01-01 00:00:00"
        ],
        [
         "planned_prd",
         "2023-07-19 00:00:00"
        ],
        [
         "batch_spd",
         null
        ],
        [
         "qi_date",
         null
        ],
        [
         "pi_terms",
         "30"
        ],
        [
         "pi_applicable",
         "1"
        ],
        [
         "ci_terms",
         "0"
        ],
        [
         "ci_applicable",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 38
       }
      },
      "text/plain": [
       "po_razin_id                     PO356103HKGA-0000015\n",
       "po_created_date                  2023-06-12 00:00:00\n",
       "po_approval_date                 2023-06-19 00:00:00\n",
       "supplier_confirmation_date       2023-06-21 00:00:00\n",
       "pi_invoice_approval_date         2023-06-25 00:00:00\n",
       "pi_payment_date                  2023-08-23 00:00:00\n",
       "receive_first_prd_date           2024-06-27 00:00:00\n",
       "prd_reconfirmed_date                             NaT\n",
       "po_im_date_value                 2024-02-22 00:00:00\n",
       "po_sm_date_value                 2024-02-22 00:00:00\n",
       "batch_created_ts                                 NaT\n",
       "sm_signoff_ts                                    NaT\n",
       "ci_invoice_approval_date                         NaT\n",
       "ci_payment_date                                  NaT\n",
       "qc_schedule_date                                 NaT\n",
       "ffw_booking_ts                                   NaT\n",
       "spd_ts                                           NaT\n",
       "stock_pickup_date                                NaT\n",
       "shipment_creation_date                           NaT\n",
       "shipment_in_transit_date                         NaT\n",
       "bi_invoice_approval_date                         NaT\n",
       "bi_payment_date                                  NaT\n",
       "ffwp_telex_release_date                          NaT\n",
       "shipment_stock_delivery_date                     NaT\n",
       "item_receipt_date                                NaT\n",
       "actual_cargo_pick_up_date                        NaT\n",
       "actual_shipping_date                             NaT\n",
       "actual_arrival_date                              NaT\n",
       "actual_delivery_date                             NaT\n",
       "first_prd_date                   2025-01-01 00:00:00\n",
       "final_prd_date                   2025-01-01 00:00:00\n",
       "planned_prd                      2023-07-19 00:00:00\n",
       "batch_spd                                        NaT\n",
       "qi_date                                          NaN\n",
       "pi_terms                                          30\n",
       "pi_applicable                                      1\n",
       "ci_terms                                           0\n",
       "ci_applicable                                      0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error evaluating expression 'max(prd_reconfirmed_date, add_days(receive_first_prd_date, 7))': NaTType does not support strftime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Calculation error: max(prd_reconfirmed_date, add_days(receive_first_prd_date, 7)) (NaTType does not support strftime)\n"
     ]
    }
   ],
   "source": [
    "expr = runner.calculator.config.stages['10'].fallback_calculation.expression\n",
    "result, formula = runner.calculator._evaluate_expression(expr, y)\n",
    "print(result, formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Calculation returned non-datetime: ['5'] = ['5']\n"
     ]
    }
   ],
   "source": [
    "expr = \"['5']\"  # or \"cond(1, 2, 3)\"\n",
    "result, formula = runner.calculator._evaluate_expression(expr, y)\n",
    "print(result, formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
